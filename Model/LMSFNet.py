import numbers
import os
import random
from datetime import datetime
from einops import rearrange
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.backends.cudnn as cudnn
from torch import optim
from torch.cuda.amp import autocast, GradScaler
from tensorboardX import SummaryWriter

from Src.utils.Dataloader import get_loader, test_dataset
from utils.utils import clip_gradient
from lib.pvtv2 import pvt_v2_b2


# -----------------------------
# SELA æ³¨æ„åŠ›
# -----------------------------
class SELA(nn.Module):
    def __init__(self, in_channels, out_channels, reduction=8):
        super(SELA, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 1)
        self.se = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(out_channels, out_channels // reduction, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels // reduction, out_channels, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.conv1(x)
        weight = self.se(x)
        return x * weight


# -----------------------------
# MS_SSEP å°ç›®æ ‡å¢å¼ºæ¨¡å—
# -----------------------------
class MS_SSEP(nn.Module):
    def __init__(self, in_channels, out_channels, sobel=True, lambda_edge=0.2):
        super(MS_SSEP, self).__init__()
        self.conv1 = nn.Conv2d(in_channels * 2, out_channels, 3, padding=1)
        self.conv3 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        self.se = SELA(out_channels, out_channels)

        self.sobel = sobel
        self.lambda_edge = lambda_edge

        # Sobel å·ç§¯æ ¸ï¼ˆL1 å½’ä¸€åŒ–ï¼‰
        kx = torch.tensor([[1, 0, -1],
                           [2, 0, -2],
                           [1, 0, -1]], dtype=torch.float32) / 8.0
        ky = torch.tensor([[1, 2, 1],
                           [0, 0, 0],
                           [-1, -2, -1]], dtype=torch.float32) / 8.0
        self.register_buffer('sobel_x', kx.view(1, 1, 3, 3))
        self.register_buffer('sobel_y', ky.view(1, 1, 3, 3))

    def forward(self, x_low, x_high):
        """
        å’Œä½ åŸæ¥çš„è°ƒç”¨æ–¹å¼ä¸€è‡´ï¼š
            x2_t = x2_t + self.ssep_2(x1_down, x2_t)
        æ‰€ä»¥è¿™é‡Œå‚æ•°é¡ºåºæ˜¯ (x_low, x_high)
        """
        x = torch.cat([x_low, x_high], dim=1)  # [B, 2C, H, W]
        x = self.conv1(x)
        x = self.conv3(x)
        x = self.se(x)

        if self.sobel:
            # è¾¹ç¼˜å¼•å¯¼
            gray = x.mean(1, keepdim=True)
            kx = self.sobel_x.to(x.dtype)
            ky = self.sobel_y.to(x.dtype)
            gray_pad = F.pad(gray, (1, 1, 1, 1), mode="replicate")
            gx = F.conv2d(gray_pad, kx)
            gy = F.conv2d(gray_pad, ky)
            edge = torch.sqrt(gx * gx + gy * gy + 1e-6)

            B = edge.shape[0]
            e_flat = edge.view(B, -1)
            k = (e_flat.shape[1] * 95) // 100 + 1
            q95 = e_flat.kthvalue(k, dim=1).values.view(B, 1, 1, 1)
            edge_n = (edge / (q95 + 1e-6)).clamp(0, 1).detach()

            x = x * (1.0 + self.lambda_edge * edge_n)

        return x


class GLCF(nn.Module):
    def __init__(self, channel):
        super(GLCF, self).__init__()
        self.att_local = nn.Sequential(
            nn.Conv2d(channel * 4, channel, 1),
            nn.BatchNorm2d(channel),
            nn.ReLU(inplace=True)
        )
        self.att_global = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channel * 4, channel // 4, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(channel // 4, channel * 4, 1),
            nn.Sigmoid()
        )
        self.global_reduce = nn.Conv2d(channel * 4, channel, 1)  # ğŸ‘ˆ æ·»åŠ è¿™ä¸€å±‚

    def forward(self, x4, x3, x2, x1):
        x_all = torch.cat([
            F.interpolate(x4, size=x1.size()[2:], mode='bilinear', align_corners=True),
            F.interpolate(x3, size=x1.size()[2:], mode='bilinear', align_corners=True),
            F.interpolate(x2, size=x1.size()[2:], mode='bilinear', align_corners=True),
            x1
        ], dim=1)  # shape: [B, channel*4, H, W]

        local_feat = self.att_local(x_all)
        global_weight = self.att_global(x_all)
        global_feat = x_all * global_weight  # [B, channel*4, H, W]

        global_feat = self.global_reduce(global_feat)  # ğŸ‘ˆ é™ç»´åˆ° channel

        fused = local_feat + global_feat
        return fused


class WithBias_LayerNorm(nn.Module):
    def __init__(self, normalized_shape, eps=1e-6):
        super(WithBias_LayerNorm, self).__init__()
        if isinstance(normalized_shape, numbers.Integral):
            normalized_shape = (normalized_shape,)
        normalized_shape = torch.Size(normalized_shape)

        assert len(normalized_shape) == 1
        # ä¿æŒä½ åŸæ¥çš„åˆå§‹åŒ–æ–¹å¼
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))
        self.normalized_shape = normalized_shape
        self.eps = eps  # æ·»åŠ epsilonå‚æ•°ï¼Œé¿å…é™¤é›¶

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼šå¸¦å¯å­¦ä¹ åç½®å’Œæƒé‡çš„LayerNorm
        è¾“å…¥xçš„shapeä¸º [B, N, C]ï¼ˆæ¥è‡ªLayerNormç±»çš„reshapeï¼‰
        """
        # å¯¹æœ€åä¸€ç»´ï¼ˆç‰¹å¾ç»´åº¦Cï¼‰è¿›è¡Œå½’ä¸€åŒ–
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)

        # LayerNormæ ¸å¿ƒè®¡ç®—
        x = (x - mean) / torch.sqrt(var + self.eps)

        # åº”ç”¨å¯å­¦ä¹ çš„æƒé‡å’Œåç½®
        x = self.weight * x + self.bias

        return x


class LayerNorm(nn.Module):
    def __init__(self, dim):
        super(LayerNorm, self).__init__()
        self.body = WithBias_LayerNorm(dim)

    def forward(self, x):
        # å½“å‰çš„x.shapeçš„å€¼æ˜¯ [16, 256, 96, 96]
        h, w = x.shape[-2:]
        x = rearrange(x, 'b c h w -> b (h w) c')
        x = self.body(x)
        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)
        return x


class Interaction(nn.Module):
    # dim æ˜¯è¾“å…¥ç‰¹å¾å›¾çš„é€šé“æ•° 256
    # num_heads æ³¨æ„åŠ›å¤´çš„æ•°é‡ 4
    # bias å°±æ˜¯å·ç§¯å±‚æ˜¯å¦ä½¿ç”¨åç½®é¡¹  false
    def __init__(self, dim, num_heads, bias):
        super(Interaction, self).__init__()
        self.num_heads = num_heads
        # è¿™é‡Œå®šä¹‰äº†ä¸€ä¸ªç”¨äºæ§åˆ¶æ³¨æ„åŠ›å€¼çš„ç¼©æ”¾å› å­
        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))
        # è¿™é‡Œå°±æ˜¯è¿›è¡Œä¸‰ä¸ª1*1çš„å·ç§¯å±‚ åˆ†åˆ«ç”¨æ¥ç”Ÿæˆ K,Q,Vç‰¹å¾å›¾
        self.qkv_0 = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)
        self.qkv_1 = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)
        self.qkv_2 = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)
        # è¿™é‡Œçš„ä¸‰ä¸ªå·ç§¯å±‚æ˜¯ä¸ºäº†å¯¹æŸ¥è¯¢ã€K,Q,Vè¿›è¡Œæ›´é«˜å±‚æ¬¡çš„ç‰¹å¾è½¬æ¢ï¼Œä½¿ç”¨groups=dim
        # æ„å‘³ç€æ¯ä¸ªé€šé“ç‹¬ç«‹è¿›è¡Œå·ç§¯æ“ä½œã€‚
        self.qkv1conv = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, groups=dim, bias=bias)
        self.qkv2conv = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, groups=dim, bias=bias)
        self.qkv3conv = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, groups=dim, bias=bias)
        # å°†è¾“å‡ºç‰¹å¾å›¾çš„é€šé“æ•°æ¢å¤åˆ°dim

        self.project_out = nn.Conv2d(dim, dim, kernel_size=3, padding=1, bias=bias)
        # å‡å°‘è¾“å‡ºçš„é€šé“æ•°ï¼Œå‹ç¼©ä¸ºåŸæ¥çš„å››åˆ†ä¹‹ä¸€
        self.compress = nn.Conv2d(dim, dim // 4, kernel_size=1, padding=0, bias=bias)
        # å±‚å½’ä¸€åŒ–ï¼Œç”¨æ¥å¯¹è¾“å…¥ç‰¹å¾å›¾è¿›è¡Œæ ‡å‡†åŒ–
        self.norm = LayerNorm(dim)

    def forward(self, x):
        # è¿™æ˜¯Interactionä¸­çš„x.shape torch.Size([12, 256, 96, 96])
        b, c, h, w = x.shape
        # xæ˜¯è¾“å…¥çš„ç‰¹å¾å›¾ï¼Œå¤§å°ä¸º[batchsize,channels,height,width] å¯¹å…¶è¿›è¡Œå½’ä¸€åŒ–
        x = self.norm(x)
        # è¾“å‡ºxè¿˜æ˜¯[12,256,96,96]ï¼Œä½†æ˜¯æ¯ä¸ªé€šé“çš„æ¯ä¸ªåƒç´ å€¼ä¼šæ ¹æ®è¯¥é€šé“çš„å‡å€¼å’Œæ ‡å‡†å·®
        # è¿›è¡Œå½’ä¸€åŒ–å¤„ç†

        # é€šè¿‡qkv_0ã€qkv_1ã€qkv_2å·ç§¯å±‚ åˆ†åˆ«æŸ¥è¯¢q ã€k ã€ v ç‰¹å¾å›¾
        # è¿™é‡Œçš„xè¿˜æ˜¯ 12ï¼Œ256ï¼Œ96ï¼Œ96 ç»è¿‡1*1çš„å·ç§¯ï¼Œç„¶åç»è¿‡3*3çš„å·ç§¯
        q = self.qkv1conv(self.qkv_0(x))
        k = self.qkv2conv(self.qkv_1(x))
        v = self.qkv3conv(self.qkv_2(x))

        # è°ƒæ•´å¼ é‡å½¢çŠ¶
        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)
        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)
        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)

        # å½’ä¸€åŒ–
        q = torch.nn.functional.normalize(q, dim=-1)
        k = torch.nn.functional.normalize(k, dim=-1)

        # è®¡ç®—æ³¨æ„åŠ›
        attn = (q @ k.transpose(-2, -1)) * self.temperature
        attn = attn.softmax(dim=-1)

        # æ³¨æ„åŠ›åŠ æƒ
        out = (attn @ v)
        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)

        # æ®‹å·®è¿æ¥ + å‹ç¼©
        out = self.project_out(out) + x
        out = self.compress(out)

        return out


class LMSFNet(nn.Module):
    """
    - backbone: pvt_v2_b2ï¼Œè¾“å‡º x1,x2,x3,x4
    - Translayer*ï¼šç»Ÿä¸€åˆ° fuse_channels ç»´åº¦
    - MSSEPï¼šx1->x2, x1->x3 çš„å°ç›®æ ‡å¢å¼º
    - èåˆï¼šx1, x2', x3', x4' ä¸Šé‡‡æ ·åˆ°åŒä¸€å°ºå¯¸ concatï¼Œå† conv èåˆ -> seg_head è¾“å‡º
    - æœ€åå†ä¸Šé‡‡æ ·åˆ°è¾“å…¥å¤§å°
    """

    def __init__(self,
                 pretrained_pvt_path=None,
                 out_channels=1,
                 fuse_channels=64,
                 use_mssep=True):
        super().__init__()
        self.backbone = pvt_v2_b2()
        self.use_mssep = use_mssep
        self.fuse_channels = fuse_channels

        # --- åŠ è½½ PVT é¢„è®­ç»ƒ ---
        if pretrained_pvt_path is not None:
            try:
                state_dict = torch.load(pretrained_pvt_path, map_location="cpu")
                model_dict = self.backbone.state_dict()
                state_dict = {k: v for k, v in state_dict.items() if k in model_dict}
                model_dict.update(state_dict)
                self.backbone.load_state_dict(model_dict)
                print(f"[Backbone] Loaded PVTv2-B2 weights from {pretrained_pvt_path}")
            except Exception as e:
                print(f"[Backbone] WARNING: failed to load PVTv2-B2 weights: {e}")

        # pvt_v2_b2 è¾“å‡ºé€šé“: [64, 128, 320, 512]
        self.trans1 = nn.Conv2d(64, fuse_channels, kernel_size=1)
        self.trans2 = nn.Conv2d(128, fuse_channels, kernel_size=1)
        self.trans3 = nn.Conv2d(320, fuse_channels, kernel_size=1)
        self.trans4 = nn.Conv2d(512, fuse_channels, kernel_size=1)

        # MSSEPï¼ˆåªåœ¨ use_mssep=True çš„æ—¶å€™ç”¨ï¼‰
        if self.use_mssep:
            self.ssep_2 = MS_SSEP(fuse_channels, fuse_channels)
            self.ssep_3 = MS_SSEP(fuse_channels, fuse_channels)

        # å¤šå°ºåº¦èåˆï¼š4 ä¸ªå°ºåº¦ concat åå·ç§¯
        self.fuse_conv = nn.Sequential(
            nn.Conv2d(fuse_channels * 5, fuse_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(fuse_channels, fuse_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
        )
        self.intra = Interaction(64 * 4, 4, False)
        self.glcf = GLCF(64)
        # segmentation head
        self.seg_head = nn.Conv2d(fuse_channels, out_channels, kernel_size=1)

    def forward(self, x):
        H, W = x.shape[2], x.shape[3]

        # 1) backbone æå–å¤šå°ºåº¦ç‰¹å¾
        x1, x2, x3, x4 = self.backbone(x)  # [B,64,H/4,W/4], [B,128,H/8,W/8]...

        # 2) é€šé“ç»Ÿä¸€
        x1_t = self.trans1(x1)  # [B,C,H/4,W/4]
        x2_t = self.trans2(x2)  # [B,C,H/8,W/8]
        x3_t = self.trans3(x3)  # [B,C,H/16,W/16]
        x4_t = self.trans4(x4)  # [B,C,H/32,W/32]

        # 3) MSSEP å°ç›®æ ‡å¢å¼º
        if self.use_mssep:
            # x1 -> x2
            x1_down2 = F.interpolate(x1_t, size=x2_t.shape[2:], mode='bilinear', align_corners=True)
            x2_t = x2_t + self.ssep_2(x1_down2, x2_t)

            # x1 -> x3
            x1_down3 = F.interpolate(x1_t, size=x3_t.shape[2:], mode='bilinear', align_corners=True)
            x3_t = x3_t + self.ssep_3(x1_down3, x3_t)

        # 4) å¤šå°ºåº¦äº¤äº’ï¼šå…¨éƒ¨ä¸Šé‡‡æ ·åˆ° x1_t çš„åˆ†è¾¨ç‡ï¼Œå† concat
        x_qkv = self.intra(torch.cat((x1, F.interpolate(x2_t, size=x1.size()[2:], mode='bilinear'),
                                      F.interpolate(x3_t, size=x1.size()[2:], mode='bilinear'),
                                      F.interpolate(x4_t, size=x1.size()[2:], mode='bilinear')), 1))
	#5)ä¸Šä¸‹æ–‡èåˆ
        x_share = self.glcf(x4_t, x3_t, x2_t, x1)
        x_share = x_qkv + x_share
        target_size = x1_t.shape[2:]  # H/4, W/4
        f1 = x1_t
        f2 = F.interpolate(x2_t, size=target_size, mode='bilinear', align_corners=True)
        f3 = F.interpolate(x3_t, size=target_size, mode='bilinear', align_corners=True)
        f4 = F.interpolate(x4_t, size=target_size, mode='bilinear', align_corners=True)

        feats_cat = torch.cat([f1, f2, f3, f4, x_share], dim=1)  # [B, 4C, H/4, W/4]
        fused = self.fuse_conv(feats_cat)  # [B, C, H/4, W/4]

        # 6) è¾“å‡ºé¢„æµ‹ + ä¸Šé‡‡æ ·åˆ°è¾“å…¥å¤§å°
        logits = self.seg_head(fused)  # [B, 1, H/4, W/4]
        logits = F.interpolate(logits, size=(H, W), mode='bilinear', align_corners=False)  # [B,1,H,W]

        return logits
